# Logistic regression example
  - simplified example of `logistic regression` with `one data point` to illustrate the main ideas of the `forward pass`, `loss calculation`, and `backpropagation (gradient descent)`

# Scenario
  - We want to predict if a student will pass an exam based on the number of hours they studied.

# Dataset (One Data Point)
  - Input (Hours Studied): x = 2
  - Output (Pass/Fail): y = 1 (Pass)

# Model
  - Our logistic regression model will have `one weight (w)` for the input feature and `a bias (b)`

# The prediction process
  - Linear Combination: $z = w * x + b$
  - Sigmoid Activation: $p = σ(z) = \frac{1}{1+e^{-z}}$

# Initial Parameters (Let's start with some arbitrary values)
  - Weight (w) = 0.5
  - Bias (b) = -1

# Step 1: Forward Pass
  - Calculate the linear combination (z)
  - $z=w * x + b = 0.5 * 2 + (-1) = 1 + (-1) = 0$
  - $p = σ(z) = \frac{1}{1+e^{-z}} = \frac{1}{1+e^{0}} = \frac{1}{1+1} = \frac{1}{2} = 0.5$

# Step 2: Calculate the Loss
  - We'll use the binary cross-entropy (log loss) function
  - $loss(p, y) = - (y * log(p) + (1 - y) * log(1 - p))$
  - $loss(0.5, 1) ≈ 0.693$

# Step 3: Backpropagation (Gradient Descent)

## 1. Calculate the gradient of the loss with respect to the prediction (dp)
  - $\frac{d(loss)}{dp} = \frac{1-y}{1-p} - \frac{y}{p} = -2$

## 2. Calculate the gradient of the prediction with respect to the linear combination (dz):
  - $\frac{dp}{dz} = p * (1 - p) = 0.25$
  - This is the derivative of the sigmoid function

## 3. Calculate the gradient of the loss with respect to the linear combination (dz)
  - $\frac{d(loss)}{dz} = \frac{d(loss)}{dp} * \frac{dp}{dz} = -0.5$
  - Chain Rule

## 4. Calculate the gradient of the loss with respect to the weight (dw)
  - $\frac{d(loss)}{dw} = \frac{d(loss)}{dz} * \frac{dz}{dw} = -1$
  - $\frac{dz}{dw} = x$ (The derivative of $w*x + b$ with respect to $w$ is $x$)

## 5. Calculate the gradient of the loss with respect to the bias (db)
  - $\frac{d(loss)}{db} = \frac{d(loss)}{dz} * \frac{dz}{db} = -0.5$
  - $\frac{dz}{db} = 1$ (The derivative of $w*x + b$ with respect to $b$ is $1$)

# Step 4: Update Parameters
We update the weights and bias using a learning rate (α). Let's say our learning rate is 0.1.

## Update weight (w):
  - $w_{new} = w_{old} - α * \frac{d(loss)}{dw} = 0.6$

## Update bias (b)
  - b_new = b_old - α * d(loss)/db
  - b_new = -1 - 0.1 * (-0.5)
  - b_new = -1 + 0.05
  - b_new = -0.95