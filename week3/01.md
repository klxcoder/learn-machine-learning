# Logistic regression example
  - simplified example of `logistic regression` with `one data point` to illustrate the main ideas of the `forward pass`, `loss calculation`, and `backpropagation (gradient descent)`

# Scenario
  - We want to predict if a student will pass an exam based on the number of hours they studied.

# Dataset (One Data Point)
  - Input (Hours Studied): x = 2
  - Output (Pass/Fail): y = 1 (Pass)

# Model
  - Our logistic regression model will have `one weight (w)` for the input feature and `a bias (b)`

# The prediction process
  - Linear Combination: $z = w * x + b$
  - Sigmoid Activation: $p = σ(z) = \frac{1}{1+e^{-z}}$

# Initial Parameters (Let's start with some arbitrary values)
  - Weight (w) = 0.5
  - Bias (b) = -1

# Step 1: Forward Pass
  - Calculate the linear combination (z)
  - $z=w * x + b = 0.5 * 2 + (-1) = 1 + (-1) = 0$
  - $p = σ(z) = \frac{1}{1+e^{-z}} = \frac{1}{1+e^{0}} = \frac{1}{1+1} = \frac{1}{2} = 0.5$

# Step 2: Calculate the Loss
  - We'll use the binary cross-entropy (log loss) function:
  - $loss(p, y) = - (y * log(p) + (1 - y) * log(1 - p))$
  - $loss(0.5, 1) ≈ 0.693$

# Step 3: Backpropagation (Gradient Descent)

## 1. Calculate the gradient of the loss with respect to the prediction (da)
  - dLoss/da = - [y/a - (1-y)/(1-a)]
  - dLoss/da = - [1/0.5 - (0)/(0.5)]
  - dLoss/da = - [2 - 0]
  - dLoss/da = -2

## 2. Calculate the gradient of the prediction with respect to the linear combination (dz):
  - da/dz = a * (1 - a)  (This is the derivative of the sigmoid function)
  - da/dz = 0.5 * (1 - 0.5)
  - da/dz = 0.5 * 0.5
  - da/dz = 0.25

## 3. Calculate the gradient of the loss with respect to the linear combination (dz)
  - dLoss/dz = dLoss/da * da/dz (Chain Rule)
  - dLoss/dz = -2 * 0.25
  - dLoss/dz = -0.5

## 4. Calculate the gradient of the loss with respect to the weight (dw)
  - dLoss/dw = dLoss/dz * dz/dw
  - dz/dw = x (The derivative of w*x + b with respect to w is x)
  - dLoss/dw = -0.5 * 2
  - dLoss/dw = -1

## 5. Calculate the gradient of the loss with respect to the bias (db)
  - dLoss/db = dLoss/dz * dz/db
  - dz/db = 1 (The derivative of w*x + b with respect to b is 1)
  - dLoss/db = -0.5 * 1
  - dLoss/db = -0.5

# Step 4: Update Parameters
We update the weights and bias using a learning rate (α). Let's say our learning rate is 0.1.

## Update weight (w):
  - w_new = w_old - α * dLoss/dw
  - w_new = 0.5 - 0.1 * (-1)
  - w_new = 0.5 + 0.1
  - w_new = 0.6

## Update bias (b)
  - b_new = b_old - α * dLoss/db
  - b_new = -1 - 0.1 * (-0.5)
  - b_new = -1 + 0.05
  - b_new = -0.95